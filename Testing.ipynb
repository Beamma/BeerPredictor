{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "* Download the files, and extract them and write them into the required form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Download gzip file\n",
    "filename = 'ratebeer.json.gz'\n",
    "urllib.request.urlretrieve('https://datarepo.eng.ucsd.edu/mcauley_group/data/beer/ratebeer.json.gz', filename)\n",
    "\n",
    "# Extract gzip file into a json file\n",
    "def unzip_gzip(input_file, output_file):\n",
    "    with gzip.open(input_file, 'rb') as f_in:\n",
    "        with open(output_file, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "def json_to_jsonlines(input_file):\n",
    "   jsonHolder =  []\n",
    "   with open(input_file, 'r') as input_file:\n",
    "     for obj in input_file:\n",
    "       try:\n",
    "         data_dict = json.loads(obj.replace(\"'\", \"\\\"\"))\n",
    "         jsonHolder.append({\"review/profileName\": data_dict[\"review/profileName\"], \"rating\": data_dict['review/overall'], \"beer/beerId\": data_dict[\"beer/beerId\"], \"name\": data_dict[\"beer/name\"]})\n",
    "       except:\n",
    "         pass\n",
    "   return jsonHolder\n",
    "\n",
    "def convert_to_jsonl():\n",
    "    with open('data.jsonl', 'w') as f:\n",
    "      for entry in jsonHolder:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "unzip_gzip('ratebeer.json.gz', 'data.json')\n",
    "jsonHolder = json_to_jsonlines('data.json')\n",
    "convert_to_jsonl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Clusters and run the beer prediction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Dashboard link:  http://127.0.0.1:8787/status\n",
      "Number of entries after filtering: 1947817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/csse/users/jbr257/.local/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 205.83 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "2024-06-03 03:05:42,249 - distributed.utils_perf - WARNING - full garbage collections took 19% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique beers: 8963\n",
      "Number of unique users: 6479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/csse/users/jbr257/.local/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 206.03 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "2024-06-03 03:06:49,115 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:06:51,262 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:06:51,799 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:06:52,264 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:06:54,019 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:06:55,195 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:06:56,148 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:06:57,817 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:06:58,726 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:06:59,773 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:00,063 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:00,399 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:01,094 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:02,630 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:03,917 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:12,350 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:13,272 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:14,385 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:15,775 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:17,596 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:19,675 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:22,272 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:25,489 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-06-03 03:07:29,486 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "/csse/users/jbr257/.local/lib/python3.10/site-packages/distributed/client.py:3161: UserWarning: Sending large graph of size 443.18 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Westvleteren Extra 8\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster, default_client\n",
    "import dask\n",
    "from dask import bag as db\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Start up clusters\n",
    "try:\n",
    "    if default_client() is not None:\n",
    "        default_client().close()\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "cluster = LocalCluster(n_workers=7, threads_per_worker=1, memory_limit='4GB')\n",
    "client = Client(cluster)\n",
    "dashboard_link = client.dashboard_link\n",
    "print (\"Dask Dashboard link: \", dashboard_link)\n",
    "\n",
    "data_bag = db.read_text('data.jsonl', blocksize=\"10MB\")\n",
    "data_bag = data_bag.map(json.loads)\n",
    "\n",
    "# data_bag = data_bag.random_sample(0.2) # For setting size of sample data to be ran\n",
    "# print(data_bag.count().compute()) # Debug\n",
    "\n",
    "# Filter the data, to only use relvant entres\n",
    "profile_name_counts = data_bag.pluck(\"review/profileName\").frequencies().compute()\n",
    "beer_id_counts = data_bag.pluck(\"beer/beerId\").frequencies().compute()\n",
    "\n",
    "profile_name_counts_dict = dict(profile_name_counts)\n",
    "beer_id_counts_dict = dict(beer_id_counts)\n",
    "\n",
    "filtered_bag = data_bag.filter(lambda x: beer_id_counts_dict[x[\"beer/beerId\"]] > 50)\n",
    "filtered_bag = filtered_bag.filter(lambda x: profile_name_counts_dict[x[\"review/profileName\"]] > 20)\n",
    "\n",
    "\n",
    "result = filtered_bag.compute()\n",
    "\n",
    "print(f\"Number of entries after filtering: {len(result)}\") # Debug\n",
    "\n",
    "\n",
    "data_bag = db.from_sequence(result, npartitions=16)\n",
    "\n",
    "# Get unique users and beers\n",
    "beer_with_name = data_bag.map(lambda x: (x[\"beer/beerId\"], x[\"name\"]))\n",
    "beer_with_name = beer_with_name.compute()\n",
    "beer_with_name = dict(beer_with_name)\n",
    "\n",
    "unique_profile_names = data_bag.pluck('review/profileName').distinct().compute()\n",
    "unique_beer_ids = data_bag.pluck(\"beer/beerId\").distinct().compute()\n",
    "\n",
    "num_profiles = len(unique_profile_names)\n",
    "num_beers = len(unique_beer_ids)\n",
    "\n",
    "print(f\"Number of unique beers: {num_beers}\") # Debug\n",
    "print(f\"Number of unique users: {num_profiles}\") # Debug\n",
    "\n",
    "utility_matrix = [np.zeros(num_beers) for _ in range(num_profiles)]\n",
    "\n",
    "# Map the index of the userId and beerId to there position in the utility matrix\n",
    "user_index_map = {user: idx for idx, user in enumerate(unique_profile_names)}\n",
    "beer_index_map = {beer: idx for idx, beer in enumerate(unique_beer_ids)}\n",
    "\n",
    "# client.restart() # May be required\n",
    "\n",
    "def update_sinle_record(row, user_index_map, beer_index_map):\n",
    "    beer_id = beer_index_map[row['beer/beerId']]\n",
    "    profile_id = user_index_map[row['review/profileName']]\n",
    "    rating = int(row['rating'].split('/')[0])\n",
    "    return (beer_id, profile_id, rating)\n",
    "\n",
    "def apply_update(row):\n",
    "    global user_index_map\n",
    "    global beer_index_map\n",
    "    return update_sinle_record(row, user_index_map, beer_index_map)\n",
    "\n",
    "partitioned_bag = data_bag.repartition(npartitions=128)\n",
    "updates = partitioned_bag.map(apply_update).compute()\n",
    "\n",
    "for beer_id, user_id, rating in updates:\n",
    "    utility_matrix[user_id][beer_id] = rating\n",
    "\n",
    "# print(utility_matrix) # Debug\n",
    "\n",
    "def clear_worker_data():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "client.run(clear_worker_data)\n",
    "client.rebalance()\n",
    "\n",
    "test_user = utility_matrix[0] # Set test user to the first user in matrix\n",
    "# print(test_user) # Debug\n",
    "\n",
    "utility_matrix_bag = db.from_sequence(utility_matrix)\n",
    "\n",
    "# client.restart()\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    # print(f\"{dot_product} / ({norm_u} * {norm_v})\")\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    # print(similarity)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def calculate_similarity(utility_matrix, test_user):\n",
    "    sims = utility_matrix.map(lambda x: (x, cosine_similarity(x, test_user)))\n",
    "    return sims\n",
    "\n",
    "similarities = calculate_similarity(utility_matrix_bag, test_user)\n",
    "\n",
    "# print(similarities.compute()) # Debug\n",
    "\n",
    "top_similar_users = sorted(similarities, key=lambda x: x[1], reverse=True)[1:11]\n",
    "# print(top_similar_users) # Debug\n",
    "\n",
    "def get_weighted_matrix(similar_users, utility_matrix):\n",
    "    weighted_matrix = []\n",
    "    for user_tup in similar_users:\n",
    "        similarity = user_tup[1]\n",
    "        user_matrix = user_tup[0]\n",
    "        for i in range(len(user_matrix)):\n",
    "            user_matrix[i] = similarity * user_matrix[i]\n",
    "        weighted_matrix.append((user_id, user_matrix))\n",
    "    \n",
    "    return weighted_matrix\n",
    "\n",
    "weighted_matrix = get_weighted_matrix(top_similar_users, utility_matrix)\n",
    "\n",
    "# print(weighted_matrix) # Debug\n",
    "\n",
    "def recommend_beer(weighted_matrix, num_similar, num_beers):\n",
    "    weighted_sum = np.zeros(num_beers)\n",
    "    for i in range(num_similar):\n",
    "        for j in range(num_beers):\n",
    "            weighted_sum[j] = weighted_sum[j] + weighted_matrix[i][1][j]\n",
    "    return weighted_sum\n",
    "\n",
    "num_similar = len(top_similar_users)\n",
    "num_beers = len(utility_matrix[0])\n",
    "    \n",
    "weighted_sum = recommend_beer(weighted_matrix, num_similar, num_beers)\n",
    "\n",
    "print(beer_with_name[unique_beer_ids[np.argmax(weighted_sum)]]) # Print the recommended bear\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
