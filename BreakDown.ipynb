{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following note book outlines the broken down code of the algorithm\n",
    "<br/>\n",
    "The below code downloads the dataset in the form a gzip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "filename = 'ratebeer.json.gz'\n",
    "urllib.request.urlretrieve('https://datarepo.eng.ucsd.edu/mcauley_group/data/beer/ratebeer.json.gz', filename) # Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code extracts the gziped json file into a regular .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def unzip_gzip(input_file, output_file):\n",
    "    with gzip.open(input_file, 'rb') as f_in:\n",
    "        with open(output_file, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "unzip_gzip('ratebeer.json.gz', 'data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code takes the json file, and stores it as a list of python dicionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def json_to_jsonlines(input_file):\n",
    "   jsonHolder =  []\n",
    "   with open(input_file, 'r') as input_file:\n",
    "     for obj in input_file:\n",
    "       try:\n",
    "         data_dict = json.loads(obj.replace(\"'\", \"\\\"\"))\n",
    "         jsonHolder.append({\"review/profileName\": data_dict[\"review/profileName\"], \"rating\": data_dict['review/overall'], \"beer/beerId\": data_dict[\"beer/beerId\"], \"name\": data_dict[\"beer/name\"]})\n",
    "       except:\n",
    "         pass\n",
    "   return jsonHolder\n",
    "\n",
    "jsonHolder = json_to_jsonlines(\"data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code takes takes the stored python dictionaries, and writes this out to a json lines file (.jsonl) a basic json file with each entry on a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_jsonl():\n",
    "    with open('data.jsonl', 'w') as f:\n",
    "      for entry in jsonHolder:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "convert_to_jsonl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code starts up the dask distributed clusters. Here is where we set the amount of workers, the amount of threads these workers use and the memory limit for each worker. It also starts up the dask dashboard, that shows the tasks and workers as they work in parralell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster, default_client\n",
    "\n",
    "# If a client is already running, close it\n",
    "try:\n",
    "    if default_client() is not None:\n",
    "        default_client().close()\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "cluster = LocalCluster(n_workers=8, threads_per_worker=1, memory_limit='4GB') # Set number of workers, threads, and max memory for each worker\n",
    "client = Client(cluster)\n",
    "dashboard_link = client.dashboard_link\n",
    "print (\"Dask Dashboard link: \", dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is used for reading the json lines file into a dask bag of dictionaries. Here we can also set our data set sample size.\n",
    "Comment out the random_sample line in order to perform the analysis on all reviews in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import bag as db\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "data_bag = db.read_text('data.jsonl', blocksize=\"10MB\")\n",
    "data_bag = data_bag.map(json.loads)\n",
    "\n",
    "data_bag = data_bag.random_sample(0.1) # Set dataset sample size, comment this line out for the whole dataset\n",
    "print(f\"You are preforming the algorithm on {data_bag.count().compute()} unique reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is used for filtering out that may cause sparity issues. It works by first getting the frequences of occurance for the beerIds and users, and stores these in corresponding python dictionaries. The dataset is then filtered to remove any beers or profiles, that dont meet the required threshold. I chose 50 for the count of beers, as in a data set of over 2.8 million reviews, a beer that occurs less than 50 times is extremely unlikely to be the predicted beer, and if it is, its unlikely that the user will actually enjoy it. I chose a profile threshold of 10, as if a user has reviewed less that 10 beers its unlikely that they have a distinguishable pattern. Another reason for setting this threshold is that it reduces the likely hood of the user having the same binary matrix any other user. Which is good cause if this is the case, then there will be no predicted beer for that user, and they have both had only the same beers, thus not being able to predict one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of unique users and beers\n",
    "profile_name_counts = data_bag.pluck(\"review/profileName\").frequencies().compute()\n",
    "beer_id_counts = data_bag.pluck(\"beer/beerId\").frequencies().compute()\n",
    "\n",
    "# Store unique counts in pythn dictionaries\n",
    "profile_name_counts_dict = dict(profile_name_counts)\n",
    "beer_id_counts_dict = dict(beer_id_counts)\n",
    "\n",
    "# Filter the data set by the given thresholds for profiles and beers\n",
    "filtered_bag = data_bag.filter(lambda x: beer_id_counts_dict[x[\"beer/beerId\"]] > 50)\n",
    "filtered_bag = filtered_bag.filter(lambda x: profile_name_counts_dict[x[\"review/profileName\"]] > 10)\n",
    "\n",
    "\n",
    "result = filtered_bag.compute()\n",
    "\n",
    "print(f\"After filtering the data you are left with {len(result)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bag = db.from_sequence(result, npartitions=16)\n",
    "\n",
    "beer_with_name = data_bag.map(lambda x: (x[\"beer/beerId\"], x[\"name\"]))\n",
    "beer_with_name = beer_with_name.compute()\n",
    "beer_with_name = dict(beer_with_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code, gets all the unique profile names and beers, it then computes the length of these lists. It then creates a empty utility matrix populated with all 0s of the dimension given by the amount of users and number of beers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique names and unique beers\n",
    "unique_profile_names = data_bag.pluck('review/profileName').distinct().compute()\n",
    "unique_beer_ids = data_bag.pluck(\"beer/beerId\").distinct().compute()\n",
    "\n",
    "# Get amount of beers and users in data set\n",
    "num_profiles = len(unique_profile_names)\n",
    "num_beers = len(unique_beer_ids)\n",
    "\n",
    "# Create a empty utility matrix populates with all 0s\n",
    "utility_matrix = [np.zeros(num_beers) for _ in range(num_profiles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code, creates a mapping from the beer ids and profiles names given by the data set, to the correspoding indexes inside the utility matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_index_map = {user: idx for idx, user in enumerate(unique_profile_names)}\n",
    "beer_index_map = {beer: idx for idx, beer in enumerate(unique_beer_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code, takes the bag of user revews, the beer and profile name mapping and uses this to update the utility matrix, propulating it with the users ratings for the given beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the users real beer id, real user id and rating for each record in the data bag \n",
    "def update_sinle_record(row, user_index_map, beer_index_map):\n",
    "    beer_id = beer_index_map[row['beer/beerId']]\n",
    "    profile_id = user_index_map[row['review/profileName']]\n",
    "    rating = int(row['rating'].split('/')[0])\n",
    "    return (beer_id, profile_id, rating)\n",
    "\n",
    "def apply_update(row):\n",
    "    global user_index_map\n",
    "    global beer_index_map\n",
    "    return update_sinle_record(row, user_index_map, beer_index_map)\n",
    "\n",
    "partitioned_bag = data_bag.repartition(npartitions=128)\n",
    "updates = partitioned_bag.map(apply_update).compute()\n",
    "\n",
    "# Update the utility matrix sequentially\n",
    "for beer_id, user_id, rating in updates:\n",
    "    utility_matrix[user_id][beer_id] = rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code clears the workers memory, and helps to fix some of the memory issues I was having."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clear_worker_data():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "client.run(clear_worker_data) # Do garbage collection\n",
    "client.rebalance() # Rebalance the memory across workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below two boxes are for allowing you to select a user you want a recommended beer for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_profile_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = input(\"Please select a user name\")\n",
    "user_id = user_index_map[user_name] # Get the real index of the user\n",
    "test_user = utility_matrix[user_id] # Get the test users utility matrix, to use in cosine comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code, calculates the cosine similarity for the given test user, to every other user in the utility matrix. This then returns a list of tuples containing the utility vector of the user, and similarity rating of that user. Then takes the top 10 most similar users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates cosine similarity\n",
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity\n",
    "\n",
    "# Calls the cosine similarity function, mapping it in parrallel\n",
    "def calculate_similarity(utility_matrix, test_user):\n",
    "    sims = utility_matrix.map(lambda x: (x, cosine_similarity(x, test_user)))\n",
    "    return sims\n",
    "\n",
    "utility_matrix_bag = db.from_sequence(utility_matrix) # Convert utility matrix t oa bag, allowing it to be processed in parrallel\n",
    "similarities = calculate_similarity(utility_matrix_bag, test_user) # Calculate cosine similarity for each user in the utility matrix\n",
    "\n",
    "top_similar_users = sorted(similarities, key=lambda x: x[1], reverse=True)[1:11] # Take the top 10 most similar users. Excludes the top most similar, as this is the test user them self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes, the top 10 most similar users in a list of tuples of the form (<userVector>, <similarity>) and calculates each column in the userVector by the similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_matrix(similar_users):\n",
    "    weighted_matrix = []\n",
    "    for user_tup in similar_users: # For each of the 10 similar users\n",
    "        similarity = user_tup[1]\n",
    "        user_matrix = user_tup[0]\n",
    "        for i in range(len(user_matrix)): # For each rating in the userVector\n",
    "            user_matrix[i] = similarity * user_matrix[i] # Multiply the utility matrix score by users cosine similarity\n",
    "        weighted_matrix.append((user_id, user_matrix))\n",
    "    \n",
    "    return weighted_matrix\n",
    "\n",
    "weighted_matrix = get_weighted_matrix(top_similar_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final part of code is used for giving the beer recomendation. It takes all the weighted userVectors of the similar users, and adds these together, so each column/beer has a total sum of weighted beer ratings. Then we are able to take the highest rated, and recommend this to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_beer(weighted_matrix, num_similar, num_beers):\n",
    "    weighted_sum = np.zeros(num_beers)\n",
    "    for i in range(num_similar):\n",
    "        for j in range(num_beers):\n",
    "            weighted_sum[j] = weighted_sum[j] + weighted_matrix[i][1][j]\n",
    "    return weighted_sum\n",
    "\n",
    "num_similar = len(top_similar_users)\n",
    "    \n",
    "weighted_sum = recommend_beer(weighted_matrix, num_similar, num_beers)\n",
    "print(beer_with_name[unique_beer_ids[np.argmax(weighted_sum)]])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
